#CDH5.5.0
#This document will cover SQOOP import options
#Commands to import data from mysql database to HDFS and HIVE

*************************************************************
********************* GENERIC COMMANDS **********************
*************************************************************
##To list databases in a mysql connection

sqoop list-databases \
--connect "jdbc:mysql://quickstart.cloudera:3306" \
--username retail_dba \
--password cloudera

##To list all tables in specific mysql database

sqoop list-tables \
--connect "jdbc:mysql://quickstart.cloudera.com:3306/retail_db" \
--username retail_dba \
--password cloudera \

NOTE : When working with specific database, --connect needs to have database name

#To evaluate database tables or data

sqoop eval \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--query "select count(*) from departments"

NOTE : EVAL can be used to retrieve information from database without explicitly
       connecting to the database. Alternatively, this can be used as a tool to
       cross check import is successful or not. For Eg : source and destination
       have same number of records after import

*************************************************************
******************* IMPORTING ALL TABLES ********************
*************************************************************

#IMPORTING all tables of a database to HDFS

sqoop import-all-tables \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--warehouse-dir /user/cloudera/sqoop_import/

When importing all the tables of a database use parameter
--warehouse-dir which specifies the parent directory under which all the tables
will be imported. Every table will have its own directory with files containing
the actual table data

#IMPORTING all tables of a database to HIVE

#create a database in hive, as this will be used to import all tables
#hive> create database sqoop_import;

#This will create new directory called sqoop_import.db as hive home
#At hive home there willl be 2 types of directories :
# 1. Without .db extension : These will be one directory for each table holding
     table data in files.
# 2. With    .db extension : These will be database directories as hive home.

#To run HDFS command from inside HIVE, use "dfs <command>"

sqoop import-all-tables \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--hive-import \
--hive-overwrite \
--create-hive-table \
--hive-database sqoop_import \
--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec \
--outdir java_files

#In above scenario compare oder_items table earlier created in HDFS import with
 one HIVE IMPORT

*************************************************************
****************** IMPORTING SINGLE TABLE *******************
*************************************************************

#IMPORTING single table to HDFS

sqoop import \
--connect "jdbc:,ysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--target-dir /user/cloudera/departments

When importing single table of a database use parameter
--target-dir which specifies the table specific directory under which all the
  files with table data be created.

==> Every import will create boundary query to identify number of buckets to
    divide data into equally on primary key column.
==> If you manually want to  provide different boundary query use parameter
    --BOUNDARY-QUERY
==> If you dont want all the columns from the table, use --COLUMN parameter with
    column list seperated by comma and no spaces.
==> --TABLE and --QUERY are mutually exclusive, both can not be used in same
    SQOOP command

==> If MYSQL table doesn't have a primary key, sqoop will not work as it will
    not be able to compute boundary query.
==> Solution either user --SPLIT-BY or use only 1 mapper. Column needs to be
    indexed on which splitting is to be performed.
==> --SPLIT-BY is mandatory when doing SQOOP IMPORT using --QUERY

sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--query "select * from orders join order_items on order.order_id = order_items.order_id where \$CONDITIONS" \
--target-dir /user/cloudera/order_join \
--split-by order_id \
--num-mappers 4

#Sqoop performs highly efficient data transfers by inheriting Hadoopâ€™s
 parallelism. To help Sqoop split your query into multiple chunks that can be
 transferred in parallel, you need to include the $CONDITIONS placeholder in the
 where clause of your query. Sqoop will automatically substitute this placeholder
 with the generated conditions specifying which slice of data should be
 transferred by each individual task. While you could skip $CONDITIONS by forcing
 Sqoop to run only one job using the --num-mappers 1 parameter, such a limitation
 would have a severe performance impact.

#IMPORTING single table into HIVE

#While importing in hive, destination table can either be created on the fly or t
 able can be created in hive and then data is imported using sqoop.

1) Create table first

hive> use sqoop_import ;
hive> create table departments_hive (department_id int, department_name string);

sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--hive-import \
--hive-overwrite \
--hive_table sqoop_import.departments_hive

#When importing table in hive data is first staged in user home, so if a dir
with same name exists in user home, process might fail. Once the processing if
done and import is complete, staging dir is deleted.

2) Create table on the fly

sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--hive-import \
--hive-overwrite \
--create-hive-table \
--hive-database sqoop_import
