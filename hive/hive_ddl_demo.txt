HIVE TUTORIAL
=============

# Difference with RDBMS
1) No support for real time indexes
2) No Packages or procedures or functions
3) No Triggers
4) Partitioning available but NO range partitioning
   - In terms of Oracle there are 3 types of partitioning techniques:
     LIST - This is partitioning in hive
     HASH - This is bucketing in hive
     RANGE- Not available with hive
5) Limited data types (numeric, string, binary etc.)

# Data types in HIVE
--------------------
1) Primitive data types
    - Numeric
        :: TINYINT
        :: SMALLINT
        :: INT
        :: BIGINT
        :: FLOAT
        :: DOUBLE
        :: DECIMAL

    - String
        :: STRING
        :: VARCHAR
        :: CHAR

    - Date/Time
        :: DATE
        :: TIMESTAMP

    - Miscellaneous
        :: BOOLEAN
        :: BINARY

2) Complex data types (COLLECTION DATA TYPES)
    - Struct (record type)
    - Map (key-value pair)
    - Array (ordered sequence)

    CREATE TABLE person (
        id INT,
        phones ARRAY<INT>,
        otherDetails MAP<STRING, STRING>,
        address STRUCT<street:STRING, city:STRING, state:STRING>
    );

NOTE : When converting string to varchar, if string is bigger than the length
       of varchar specified, it silently gets truncated

       VARCHAR and CHAR can not be used in UDFs and UDAFs

       DATE types can be casted in DATE, TIMESTAMP or STRING and vice versa.

HIVE DDL
========

create database if not exists retail_ods;
create database of not exists retail_edw;

Types of tables:
1) Managed table - When drop a managed table, HDFS directory is also deleted.
2) External table - When drop a external table, only metadata from hive
                    metastore is deleted. HDFS dir and files still remain.

Why use EXTERNAL TABLE:
If a table data is being used by multiple tools, if its a managed table and
table is dropped, all the tools are affected as data is lost. Thus use external
table in such scenarios.

Physical Data Modeling in HIVE
==============================
1) Take care of the file format in which data needs to be stored
2) Convert data types to as supported in hive
3) Determine delimiters (field and line)
4) Determine value for NULL values
5) Handle data redundancy
6) Determine Partitioning/Bucketing strategy
7) Too much partitioning vs. too little partitioning

Eg : Too much partitioning means many small files, which is not good for hadoop
Too little partitioning means ETL/Reporting can go slow.

EXTRACT AND LOAD - HIVE
=======================

Data can be coming from different sources like
-RDBMS (Oracle etc)
-Closed (Mainframe etc)
-External (XML, JSON etc)

Strategies for extract and load
-Push (need to use hive or write custom MR)
-Pull (can use sqoop)
-Filter as much as possible and as early as possible

++++++++++++++++++++++++++++++++++++++++++++++++++++
SQOOP can be used to load data from RDBMS
HIVE can be used to load data already extracted to HDFS in form of flat files
MR job can be used to extend capabilities to load process
++++++++++++++++++++++++++++++++++++++++++++++++++++

CREATE TABLE
============

#MANAGED TABLE

create table deck_of_cards
(
color string,
suit string,
pip string
)
row format delimited
fields terminated by '|'
lines terminated by '\n';

NOTE:
-Above command with create a table in current database in USE.
-An empty table directory will be created in current database dir in hive home.
-Row format defines if file is DELIMITED or SerDe

To load data into this table, use LOAD command as follows:

load data local inpath '/home/cloudera/demo/data/deckofcards.txt'
into table deck_of_cards;

NOTE:
Above command will actually move deckofcards.txt from local file location
as specified to table directory in HIVE.

IMPORTANT:
If there is a mismatch in fields terminated by character or lines terminated by
character between table definition and actual file, unexpected data will be
loaded

To correct that, alter table command can be used as this would require changing
serde paramter

alter table deck_of_cards
set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES ('field.delim'='|');

#EXTERNAL TABLE

create external table deck_of_cards_external
(
color string,
suit string,
pip string
)
row format delimited
fields terminated by '|'
location '/user/hive/warehouse/sqoop_import.db/deck_of_cards';

NOTE:
As above table is external table referring to same location in HDFS
as MANAGED TABLE deck_of_cards, if this MANAGED table gets dropped, data will
also be removed from HDFS location. Next when you query EXTERNAL table, it will
NOT throw an error, query will run successfully with no result.
